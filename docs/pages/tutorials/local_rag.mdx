## Building a Local RAG System with R2R: A Step-by-Step Guide

### Introduction 

R2R, short for "RAG to Riches," is a game-changing framework that simplifies the process of building RAG applications with LLMs. With R2R, you can hit the ground running and have a system running locally in minutes, eliminating the need for complex cloud infrastructure or costly hosted services.

In this comprehensive, step-by-step tutorial, we'll guide you through the process of installing R2R, ingesting your documents, querying those docs using a local LLM, and tailoring the RAG pipeline to perfectly fit your unique requirements. By the end of this guide, you'll have a fully functional, locally-hosted, and readily deployable LLM application at your fingertips!

![Llama Coding](./llama_3.png)

### Running R2R with Docker

You can easily run R2R in a containerized environment with Docker:

1. Pull the latest R2R Docker image:

   ```bash
   docker pull emrgntcmplxty/r2r:latest
   ```

2. Choose the appropriate configuration option based on your deployment:
   - For local deployment, select `local_ollama`.
   - For cloud deployment, select `default` and pass `--env-file .env` to provide the necessary environment variables.

3. Run the R2R Docker container:

   ```bash
   docker run -d --name r2r_container -p 8000:8000 -e CONFIG_OPTION=local_ollama emrgntcmplxty/r2r:latest
   ```

   This command starts the R2R container in detached mode (`-d`), names it `r2r_container`, maps port 8000 from the container to the host (`-p 8000:8000`), and sets the configuration option to `local_ollama` using the `-e` flag.

Once the container is running, you can interact with the R2R API in the same way as described in the tutorial.

### Setting Up Your Environment

R2R supports  `ollama`, a popular tool for Local LLM inference. Ollama is provided through a connection managed by the `litellm` library.

If you wish to use Ollama, it must be installed independently. You can install Ollama by following the instructions on their [official website](https://ollama.com/) or by referring to their [GitHub README](https://github.com/ollama/ollama).

Next, let's install R2R itself. We'll use pip to manage our Python dependencies. Run the following command:

```bash
pip install --upgrade pip
pip install 'r2r[eval,local_llm]'
```

This will install R2R along with the dependencies needed to run local LLMs.

### Pipeline Configuration

Let's move on to setting up the R2R pipeline. R2R relies on a `config.json` file for defining various settings, such as embedding models and chunk sizes. By default, the `config.json` found in the R2R GitHub repository's root directory is set up for cloud-based services.

For setting up an on-premises RAG system, we need to adjust the configuration to use local resources. This involves changing the embedding provider, selecting the appropriate LLM provider, and disabling evaluations.

To streamline this process, we've provided pre-configured local settings in the [`examples/configs`](https://github.com/SciPhi-AI/R2R/tree/main/r2r/examples/configs) directory, named `local_ollama`. Here's an overview of the primary changes from the default configuration:

```json
{
  "embedding": {
    "provider": "sentence-transformers",
    "model": "all-MiniLM-L6-v2",
    "dimension": 384,
    "batch_size": 32
  },
  "evals": {
    "provider": "none",
    "frequency": 0.0
  },
  "language_model": {
    "provider": "ollama"
    // default model-name is `tinyllama-1.1b-chat-v1.0.Q2_K.gguf`
  },
  ...
}
```

You may also modify the configuration defaults for ingestion, logging, and your vector database provider in a similar manner. More information on this follows below.

This chosen config modification above instructs R2R to use the `sentence-transformers` library for embeddings with the `all-MiniLM-L6-v2` model, turns off evals, and sets the LLM provider to `ollama`. During ingestion, the default is to split documents into chunks of 512 characters with 20 characters of overlap between chunks. 

A local vector database will be used to store the embeddings. The current default is a minimal sqlite implementation, with plans to migrate the tutorial to LanceDB shortly.